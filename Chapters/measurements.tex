\chapter{Measurements}
\section{Dimensional metrology}
	The \de{dimensional metrology} is the study of geometrical measurements (such lengths, areas, roughness). The \textbf{confidence} in the dimensions of this measurement systems is critical for their operation and establishes an agreement between the vendor and the customer on the quality being traded.
	
\subsection*{Length standards}
	A \textbf{measurement standard} is a practical realization of the definition of a measurement unit. As example the first international standard of length was a platinum-iridium bar names \textit{International Prototype Metre}; now the definition is changed and a meter is the length of the path travelled by the light in a vacuum in $1/299\,792\,458s $ (and so it's now a derived unit).
	
	The idea of standard allows the definition of the traceability chains (usually consisting of 4 levels) that's transferred between levels using calibration.
	
	\begin{SCfigure}[1][bht]
		\centering \includegraphics[width=7cm]{trace-levels}
		\caption{pyramid representing the 4 levels of traceability chain.}
	\end{SCfigure}
	
	A length standard can consist of a edge gauges block of, more easily, using \textbf{line standard} which provide the reference lengths as the distance between two parallel lines on the surface of the object (so, as example, a simple ruler or calliper). The line standard is often used for calibrating the scales of optical vision system.
	
	\begin{SCfigure}[2][bht]
		\centering \includegraphics[width=4cm]{ruler}
		\caption{example of a line length standard.}
	\end{SCfigure}
	
\subsection{Displacement sensors} 
	The contact displacement sensor measure the linear position of an object by physically contacting its surface with a prove. An example of digital comparator of displacement is the \textbf{LVDT} (\textit{Linear Variable Differential Transformer}) transducer. In this case the displacement is related to the motion of a movable electromagnetic core that determines a change of mutual inductance between the primary and secondary coils that's then measured by a voltmeter.
	
	\begin{SCfigure}[2][bht]
		\centering \includegraphics[width=4cm]{lvdt}
		\caption{functional schematic of an LVDT displacement sensor.}
	\end{SCfigure}
	
	Displacement can also be measured using \textbf{encoders} that determines a (relative or absolute) position in digital format by counting the number of optical/magnetic/inductive pulses. \\
	Relative encoder are easier to implement and requires only two tracks to measure the displacement and it's direction, but every time it's necessary to restart from a known position. Considering instead an absolute encoder the minimum number of tracks to measure a length $l$ with a resolution $r$ is $\lceil \log_2 (l/2) \rceil $ (for example to measure a length of $l=1m$ with a resolution of $r=1\mu m$ the minimum number of tracks is 20); this measure relates also to the number of bits representing the measure.
	
	In general absolute encoders don't presents masks with a binary encoding, but they use the \textit{Gray code} (in order to avoid the problem of multiple wrong reads of the sensor while being near to the commutation stage) that assures only one change of bit for each step.
	
	\begin{SCfigure}[1][bht]
		\centering \includegraphics[width=8cm]{greycode}
		\caption{binary code vs Gray code masks for absolute encoders.}
	\end{SCfigure}
	
	\paragraph{Non-contact displacement sensors} The previous explained measurement method for displacement require a contact between the system and the object to be probed, but it possible to create systems contact free using the following measurement principles:
	\begin{itemize}
		\item \textbf{light interferometry}: in this case the system  consists of a laser source that splitted by a beam into two retroreflectors (as shown in figure \ref{fig:meas:interferometer}).		
		
		\begin{SCfigure}[1][bht]
			\centering \includegraphics[width=6cm]{las-sys}
			\caption{schematic representation of a laser interferometer.} \label{fig:meas:interferometer}
		\end{SCfigure}
		 
		 The intensity of the interference signal is function of the phase difference between the superimposed beams ($0^\circ$ for constructive interference, $180^\circ$ for the destructive one). The intensity of the interference signal repeats with displacements of the retroreflector equal to one half of the wavelength $\lambda$ of the light beam and, by interpolation of the phase within a fringe period, it's possible to achieve sub-nanometre resolutions. In general the resolution is great ($10nm$) with a range up to hundreds of meters (and so it has a huge dynamic range);
		 
		 \item \textbf{light intensity}: this system has an high resolution but a limited range  and uses fiber optics carrier to enlight the surface and measure the reflected light;
		 \begin{figure}[bht]
		 	\centering \includegraphics[width=7cm]{fibermeasure}
		 	\caption{schematic representation of a light intensity measurement system.}
		 \end{figure}
	 
	 	\item \textbf{trilateration}: in this case we use a line pixel array to detect the shift of the reflected light source.
	 	
	 	\begin{SCfigure}[1][bht]
	 		\centering \includegraphics[width=5cm]{trilateral}
	 		\caption{schematic representation of a trilateration measurement system.}
	 	\end{SCfigure}		
	\end{itemize}
	
\subsection{Form measurement}
	\paragraph{Straightness and roundness} The \de{straightness} is measured as lateral deviation along a displacement; in this case the reference line can be computed by interpolation (by computing as example the least-square line). With an analyses of the surface we can define the parameters such the peak-ref deviation $STR_p$, the valley-ref deviation $STR_v$, the peak-valley deviation $STR_t = STR_p + STR_v$ and the RMS deviation $STR_q$.
	
	\begin{SCfigure}[2][bht]
		\centering \includegraphics[width=6cm]{straightness}
		\caption{measurement axis and straightness profile with main parameter of this kind of form measure.}
	\end{SCfigure}
	
	Similarly we can define the same parameters for a circular surface determining so the \de{roundness} $RON$ of the piece.
	
	\textbf{Flatness} can be regarded as the straightness in two dimension (on a surface, not a axis), while \textbf{parallelism} defines a tolerance zone respect to a datum, rather than respect to the reference line.
	
	\begin{SCfigure}[2][bht]
		\centering \includegraphics[width=8cm]{flat-par}
		\caption{flatness and parallel zone tolerance for a piece.}
	\end{SCfigure}
	
	In order to asses the flatness we can use an optical flat that use the patterns of the fringes due to light interferometry to determine the flatness of the surface.
	
	\begin{SCfigure}[2][bht]
		\centering \includegraphics[width=3cm]{opt-flat}
		\caption{example of an optical flat system to measure flatness.}
	\end{SCfigure}
	
	\paragraph{Roughness} To determine the \de{surface roughness} of a surface we need to split it's topography (all wavelength) into surface texture (short wavelength) and  surface form (long wavelength associated to the baseline). In particular the roughness is a one dimensional texture measured along a single scan line.
	
	\begin{SCfigure}[2][bht]
		\centering \includegraphics[width=5cm]{rough-top}
		\caption{topology, form and texture surface for a piecea.}
	\end{SCfigure}

	Texture measure can be either be contact (stylus sliding orthogonally to the surface) or contact-less (optical analysis that's intrinsically bi-dimensional). In general non contact texture instruments provides a height map image of the surface that can be easily analysed and can convey a lot of information. In general non contact texture instruments have a limitation in the resolution $r$ (compared to the contact one) that depends on the numerical aperture $A_n = n \sin\alpha$ following the relation $r = k \lambda /A_n$ (where $k=0.61$ is the Raylight criterion,$\lambda$ is the light wavelength and $n$ is the refractive index of the optic).
	
	In general to compute the parameter $Ra$ associated to the \textbf{surface roughness} we consider the equation
	\begin{equation}
		Ra = \frac 1 L \int_0^L |z(x)| \, dx
	\end{equation}
	where $L$ is the reference length on which we compute the parameter and $z(x)$ represent the deviation of the surface from the centre line.
	
	\paragraph{Reversal principle} A simple way to separate error is to use the \de{reversal principle} that consist on a double measure reversing the measurement system.
	
	\begin{SCfigure}[2][bht]
		\centering \includegraphics[width=7cm]{rev-princ}
		\caption{application on where the reversal principle can be useful.}
		\label{fig:meas:reversal}
	\end{SCfigure}
	
	Let's consider the practical case shown in figure \ref{fig:meas:reversal} of the instrument shown that allows to compute the angle $\alpha$ of the inclined plane on which is placed by transducing the rotation in the displacement $l$. If the measurement system present an imperfection that translates to an addition of the angle $\beta$ to the measure, this offset error can be compensated using the reversal principle.\\
	Knowing that the first measure gives a displacement $l_1 = \alpha + \beta$, by reversing the orientation of the instrument the output becomes $l_2 = \alpha - \beta$: this allows to calculate more precisely the value $\alpha$ and the error $\beta$ introduced by the system as
	\[ \alpha = \frac{l_1 + l_2}{2} \qquad \ \qquad \beta = \frac{l_1-l_2}{2} \]
	Once that the error $\beta$ is computed and known there's no need to perform this operation every time we want to do a measure.

\section{Statistics and uncertainty}
	The graphical and mathematical techniques that allow to describe the behaviour of a stochastic variable are called \textbf{descriptive statistics}. In this analysis it's important to define the \de{population} as the entire set (that can be both finite or infinite) of all possible observation and the \de{sample} as a random subset drawn from a population.  A sample can be extracted
	\begin{itemize}
		\item with re-insertion, used for the \textbf{simple random sampling}, where each extract element is returned to the population (and so a re-extraction might occur). This process is good when the number $n$ of samples is way less than the size $N$ of the population;
		
		\item without re-insertion, used for the \textbf{bulk sampling}, where each extracted element reduces the remaining population and so this process is suggested when $n\simeq N$.
	\end{itemize}
	
	\paragraph{Frequency and probability} We define the \de{frequency} of a random variable as the number of times a given value is observed for a certain number of observation; from that we can define the \de{probability} as the ratio of the frequency respect the total number of observation. 
	
	When the random variable is discrete ($x\in \mathds I$) then both frequency and probability can be easily calculated, while when the variable is real evaluated ($x\in \mathds R$) we have to introduce the concept of \textbf{classes} (or bins), a set of contiguous intervals of known width (not necessary with the same width); with this definition we can define the probability as the ratio between the observation in a certain class over the whole number of observations; we can also define the \textbf{probability density} as the probability of a class divided by it's bin width. The number $n$ of bins can be computed considering the following binning formulas: 
	\begin{equation}
	\begin{aligned}
		\textrm{Scott:}& \qquad n = 3.5 s_x N^{-1/3} \\ 
		\textrm{Sturges:}& \qquad n = \big\lceil 1 + \log_2(N) \big\rceil
	\end{aligned}
	\end{equation}
	where $s_x$ is the standard deviation (that will be described later) of the sample. 

	In order to analyse frequencies/probabilities we can use graphical instruments like histograms in which the vertical axis can describe the frequency, the frequency density, the probability and the probability density.
	
	\begin{SCfigure}[1][bht]
		\centering \includegraphics[width=5cm]{histo}
		\caption{example of a histogram display the frequency of a real evaluated random variable.}
	\end{SCfigure}

\subsection{Estimators}
	\paragraph{Central tendency estimators} The \de{central tendency estimators} are used to compute the \textit{central value} of a sample/population; the first main central tendency estimator is the \de{mean} operator that's usually written as $\mu$ for population and $\overline x$ for a sample $x$ and it's defined as
	\begin{equation}
		\mu = \sum_{i=1}^n x_i f(x_i) \qquad \qquad \qquad \qquad \overline x = \sum_{i=1}^n \frac{x_i}{n}
	\end{equation}
	where $x_i$ is the value associated to the $i$-th observation (in the case of the sample mean, but equally it relates to the population). Another central estimator is the \textbf{median} defined as
	\begin{equation}
		\textrm{Med}(x) = \begin{cases}
			\dfrac{x\left(\frac n2\right) + x\left(\frac{n+1}{2}\right) }{2} \qquad & \textrm{when $n$ is even} \\
			x\left(\frac{n+1}{2}\right) \qquad & \textrm{when $n$ is odd}
		\end{cases}
	\end{equation} 
	
	As last operator we define the \textbf{mode} as the most frequent value.
	
	\paragraph{Dispersion estimators} \de{Dispersion estimators} can be used to compute \textit{how away} is spread a random variable respect to the central value. The first operator is the \de{range} that determines the width of the bin associated to a value $x_i$:
	\begin{equation}
		\textrm{rng}(x_i) = \max(x_i) - \min(x_i)
	\end{equation}
	
	A more important operator is the \de{variance} ($\sigma^2$ for population, $s^2$ for sample) that can be calculated by firstly determining the mean of the sample:
	\begin{equation}
		\sigma^2 = \frac{\sum_{i=1}^n (x_i-\mu)^2 }{n} \qquad \qquad \qquad \qquad s^2 = \frac{\sum_{i=1}^n (x_i-\overline x)^2 }{n-1}
	\end{equation}
	The \de{standard deviation} is simply the square root of the variance.
	
\subsection{Probability distributions}
	Using cumulative histograms of a population it's possible to calculate the probability of finding a value in a given range $[a,b]$ as
	\[ P(a<x<b) = f_{cd,l}(b)-f_{cd,l}(a) \]
	where so $f_{cd,l}(\cdot)$ is the \textbf{cumulative probability density function}. The problem of this relation is that the population is rarely known and the \textit{shape} (in statistical expression, the \textbf{distribution}) of the histogram can be different for different variables.
	
	Defining reference distributions in mathematical terms allows to calculate probabilities without knowing the entire population. Reference distribution are identified for both discrete and continuous random variables. In this section we will discuss only the real evaluated probability distribution because they relate better to measuring systems.
	
	\paragraph{Continuous distributions} Given a continuous random variable $x\in\mathds R$, knowing it's \de{probability density function} PDF $f_{pd}(x)$ we can compute the probability of having a value in the range $[a,b]$ as
	\begin{equation}
		p(a<x<b) = \int_a^b f_{pd}(x)\, dx
	\end{equation}
	Using cumulative distributions we can determine the \de{cumulative density function} CDF both lower tail $f_{cd,l}$ and upper tail $f_{cd,u}$ as
	\begin{equation}
	\begin{aligned}
		f_{cd,l}(x) & = \int_{-\infty}^x f_{pd}(\xi)\, d\xi \\
		f_{cd,u}(x) & = \int_x^\infty f_{pd}(\xi)\, d\xi = 1 - f_{cd,l}(x) \\
		\Rightarrow \qquad p(a<x<b) &= f_{cd,l}(b)-f_{cd,l}(a)
	\end{aligned}
	\end{equation}
	From this last equation we can see that the probability of having a precise value $x_0\in x$ in a real  evaluated random variable is always zero ($P(x=x_0) = 0$).
	
	\paragraph{Uniform distribution} The simplest continuous distribution is the \de{uniform} one that's characterized by just 2 parameters, the two bound $a$ and $b$ of the distribution. We mathematically describe a random variable $y$ as uniformly distributed in the interval $[a,b]$ by writing
	\[ y \backsim \mathcal U(a,b) \]
	where
	\begin{equation}
		f_{pd}(y) = \begin{cases}
			\dfrac{1}{b-a} \qquad& a \leq y \leq b \\0 & \textrm{otherwise}
		\end{cases}
	\end{equation}
	
	\paragraph{Normal distribution} The \de{normal distribution} is one of the most used relations and it's characterized by a mean value $\mu$ and a variance $\sigma^2$. We can define the normal distribution as
	\begin{equation}
	\begin{aligned}
		y &\backsim \mathcal N(\mu,\sigma^2) \\
		f_{pd}(y) &= \frac 1 {\sigma \sqrt{2\pi}} e ^{-\frac 1 2 \left(\frac{y-\mu}{\sigma}\right)^2}
	\end{aligned}
	\end{equation}
	We can observe that the distribution computed as $\frac{y-\mu}{\sigma}$ behaves as a normal distribution with zero mean and unit variance:
	\[ \frac{y-\mu}{\sigma} \backsim \mathcal N(0,1) \]
	
	\paragraph{Chi-square distribution} The \de{chi-square} distribution of order $k$ is a sum of $k$ normal distributed random variables squared, and so
	\[ y \backsim \mathcal X_k^2 \qquad y = z_1^2+\dots+z_k^2 \qquad \textrm{where } z_i \backsim \mathcal N(0,1) \]
	This distribution presents a mean value $\mu = k$ and a variance $\sigma^2  = 2k$ with a probability density function depending on the Euler gamma function:
	\[ f_{pd}(y) = \frac{1}{2^{k/2} \Gamma(k/2) } y^{\frac k 2 - 1} e^{-y/2} \]
	
	\paragraph{Student's T distribution} A random variables is distributed as a \de{student's T} of order $k$ when
	\[ y\backsim t_k \qquad y = \frac{z}{\sqrt{x/k}} \qquad \textrm{where }  z\backsim \mathcal N(0,1), x \backsim \mathcal X_k^2 \]
	In this case the mean value is null ($\mu = 0 $) and the variance is determined by the function $\sigma^2= \frac{k}{k-2}$; in particular the probability density function of this distribution is
	\[ f_{pd} (y) = \frac{\Gamma \left( \frac{k-1}2 \right)}{\sqrt{k\pi} \Gamma(k/2)} \frac{1}{\big[ (y^2/k) + 1  \big]^{(k+1)/2}} \]
	
	\begin{figure}[bhtp]
		\centering
		\begin{subfigure}{0.48 \linewidth}
			\centering \includegraphics[width=5cm]{pdf-uniform}
			\caption{}
		\end{subfigure}
		\begin{subfigure}{0.48 \linewidth}
			\centering \includegraphics[width=5cm]{pdf-normal}
			\caption{}
		\end{subfigure}
		\begin{subfigure}{0.48 \linewidth}
			\centering \includegraphics[width=5cm]{pdf-chi}
			\caption{}
		\end{subfigure}
		\begin{subfigure}{0.48 \linewidth}
			\centering \includegraphics[width=5cm]{pdf-stud}
			\caption{}
		\end{subfigure}
	
		\caption{probability density functions of the main distribution: (a) uniform, (b) normal, (c) chi square and (d) student's T.}
	\end{figure}

	\paragraph{Distribution of sample mean} Given a random variable $x_i \backsim N(\mu,\sigma^2)$ normally distributed with mean value $\mu$ and variance $\sigma^2$, we can see that also the mean value $\overline x$ of a sample with $n$ elements randomly taken by the population is still a random variable that's still normally distributed: $\overline x \backsim \mathcal N(\mu_{\overline x}, \sigma_{\overline x}^2)$. The mean value and the variance of this distribution can be computed considering the properties of the operators \textit{expected values} $E$ and the variance one $V$:
	\begin{align*}
		E(\overline x) & = E\left( \frac{x_1+x_2+\dots+x_n}{n}\right) = \frac 1 n\Big( E(x_1) + E(x_2) + \dots + E(x_n) \Big) \\
		& = \mu  \\
		V(\overline x) & = V\left( \frac{x_1+x_2+\dots + x_n}{n} \right) = \frac 1 {n^2} \Big( V(x_1) + V(x_2) + \dots + V(x_m) \Big) \\
		& = \frac n {n^2} V(x) = \frac{\sigma_x^2}{n}
	\end{align*}
	
	\paragraph{Central limit theorem} If $x_1,\dots,x_n$ are $n$ independent random variables equally distributed with finite expected value $E(x_i) = \mu$ and variance $V(x_i) = \sigma^2$, than the defining the random variable $y = x_1+\dots+x_n$ we can see that
	\begin{equation}
		z_n = \frac{y-n\mu}{\sqrt{n\sigma^2}} \backsim \mathcal N(0,1)
	\end{equation}
	and so the distribution $z_n$ is normally distributed.
	
	This theorem stats that the limit of sum distributions tends to be normal; in measurement we can in fact consider the action of a high number of random factors (measure noises, mechanical/electrical error, environmental noises...) that, summed up, tends to provide a normal distribution.
	
\section{Inferential statistics}
	The \de{inferential statistics} is the process for which characteristic of the population are induced from the observation of a randomly extracted sample of the population itself.
	
	Fundamental part of inferential statistics are the so called \de{statistical tests} that allows us to check if a sample is compatible (or not) with a given reference population (respect to value, variance, distributions...).\\
	Statistical tests serves to decide which of two hypothesis is more likely to be wrong,  and in particular we define
	\begin{itemize}
		\item $H_0$ as the \textbf{null hypothesis} that's the weakest (typically it says that an observation is not significant and relates to the \textit{correctness} of the statement);
		\item $H_1$ as the \textbf{alternative hypothesis} that's the strong one (related to the fact that the sample is usually not compatible with the statement formulated).
	\end{itemize}
	
	Hypothesis are collected in the so called \de{confusion matrix} (table \ref{tab:meas:confmatrix}) that relates the truthiness of $H_0$ (null hypothesis) with the decision of the outcome  (accept or reject the null hypothesis).
	
	\begin{SCtable}[0.8][bht]
		\begin{tabular}{c|c c}
			$H_0$ & True & False \\ \hline 
			Accepted & ok & missed alarm \\
			Rejected & false alarm  & ok
		\end{tabular}
		\caption{confusion matrix.} \label{tab:meas:confmatrix}
	\end{SCtable}
		
	Let's consider a sample $x_i$ of $n$ elements: we can \textit{standardize} the sample mean $\overline x$ by subtracting the expected value and dividing by the standard deviation
	\begin{equation} \label{eq:studenttest}
		t_0 = \frac{|\overline x - \mu_0|}{s/\sqrt n} \backsim t_{n-1}
	\end{equation}
	The result $t_0$ of this operation is distributed as a student's T with $n-1$ degrees of freedom and represent the so called \de{test statistic}.
	
	This value is intuitively larger when the sample mean is far from the expected value (relatively to it's standard deviation); the smaller the probability of a value larger than $t_0$ on the T distribution, the stronger is the hypothesis that the sample with mean $\overline x$ and variance $s^2$ \textbf{does not} come form a population with expected value $\mu_0$. We refer to the probability of having a value larger than $t_0$ as \de{$p$-value} (that's related to the probability of a false alarm error in the confusion matrix, also referred as type I error). The lower this value is, the less error we commit on assuming the null hypothesis $H_0$ \textbf{is not verified} and so $H_1$ is happening.
		
	\begin{example}{normally distributed sample}
		Let's consider the following sample of $n=8$ elements extracted from a population normally distributed with mean $\mu = 5$ and standard deviation $\sigma = 1$:
		\[ 4.35 \qquad 6.20 \qquad 4.24 \qquad 3.89 \qquad 4.15 \qquad 4.43 \qquad 4.44 \qquad 5.2 \]
		If we want to check if this sample is uniformly distributed or not, the first thing to do is to compute it's mean $\overline x$ and standard deviation $s$:
		\[ \overline x = \sum_{i=1}^n = \frac{x_i}{n} = 4.61 \qquad \qquad s = \sqrt{ \frac{\sum_{i=1}^n (x_i-\overline x)^2 }{n-1}} = 0.74  \]
		In order to find out if this sample randomly extracted from the population it's necessary to compute the test static $t_0$ of the sample as seen in equation \ref{eq:studenttest}:
		\[ t_0 = \frac{|\overline x - \mu|}{s/\sqrt n} = \frac{|4.61-5|}{0.74/\sqrt{8}} = 1.47 \]
		Noticing that this variable is distributed as a Student T with $n-1=7$ degrees of freedom, the double tail p-value of the test statistic can be computed using the cdf of the distribution:
		\[ \pval = 2 \big(1-f_{cd,l}(t_0)\big) = 18\% \]
		
		$18\%$ so represent the error that we commit on considering the sample as not coming from the initial population that have mean $\mu = 5$; this value is quite \textit{high} (respect to the canonical $5\%$) and so we can accept the null hypothesis (or better, we can reject the strong hypothesis) with the final result that:
		\[ H_0 : \textrm{the sample belong to the initial population ($\mu = 5$)} \]
		
		If we instead would have considered as null hypothesis that the sample has been extracted by a population with a different mean $\mu = 5.5$, the test statistic would have been $t_0 = 3.37$ associated to a $\pval = 1.2\%$, and so we can confidently reject the null hypothesis and consider the strong one:
		\[ H_1 : \textrm{the sample doesn't belong to a population having $\mu = 5.5$} \]
		
	\end{example}
	
	We can note that if we know the variance of the population, the test statistic is distributed as a standard normal and the larger $n$ is, the more the distribution $t_k$ is similar to a normal $\mathcal N(0,1)$, hence for $n>50$ we can consider the test statistic as
	\[ z_0 = \frac{|\overline x-\mu_0|}{\sigma/\sqrt n} \backsim \mathcal N(0,1) \qquad \qquad \textrm{for } n >50 \] 
	
	\paragraph{Normality checks} All inferential tests are based on the assumption of normality that can usually be expected considering the central limit theorem, but it has to be checked by using different techniques like the normality plot, the quantile-quantile plot, various distributions and the chi-square test.
			
\subsection{Chi-square test}
	
	The \de{chi-square test} is a statistical test (hence non subjective) that fixed a predefined error probability for the type I error (false alarm) and it's based on the following hypothesis for the statistical test:
	\begin{itemize}
		\item $H_0$: the sample is normally distributed (or is distributed respect to another known distribution);
		\item $H_1$: the sample is not normally distributed.
	\end{itemize}
	
	To perform this kind of test the the operation to perform are the following:
	\begin{enumerate}
		\item collect sample elements in $k$ bins that are large enough to have at leat $4,5$ observations (note that the bins can have different ranges);
		\item count the observation $O_i$ ($i=1,\dots,k$) in each bin;
		\item calculate the number of expected observation $E_i$ in each bin considering the normal distribution $\mathcal N(0,1)$ (or in general computing the expected value considering  a reference distribution);
		\item calculate the test statistic as
		\begin{equation}
			X_0^2  = \sum_{i=1}^k \frac{\big(O_i-E_i\big)^2}{E_i} \backsim \mathcal X_{k-p-1}^2
		\end{equation}
	\end{enumerate}
	The test statistic is distributed as a chi-square with $k-1-p$ degrees of freedom (where $p$ is the number of parameters in the reference distribution, and so for the normal one is $p=2$); the $p$-value of this test (associated to the probability of having a type I error) is computed as
	\begin{equation}
		\pval = \min \Big\{ f_{cd,l}\big(X_0^2\big), f_{cd,u}\big(X_0^2\big) \Big\}
	\end{equation}
	
	This test can also be performed while using a confidence interval: assigned a maximum error probability $\alpha$ of having a false alarm, then the related quantiles to have such probability are $p_1 = \alpha/2$ and $p_2 = 1-\alpha/2$ (that so determines the the coefficients $\mathcal X^2_{p_1}$, $\mathcal X^2_{p_2}$); whenever $\mathcal X^2_{p_1} < X_0^2 < \mathcal X^2_{p_2}$ there is no statistical evidence that the sample is not normally distributed.

	This statistical test can so be performed to check whenever if a sample is compatible with a normal distribution or with a different theoretical distribution, but can also related the quantiles of two different samples (in order to compare and verify them) with unknown distribution a priori. For this reason this test is also called \de{Goodness-of-Fit Test} (GoF). 	
	
\subsection{Contingency test}
	\de{Contingency tests} are performed on $n$ elements of population and are used to verify if the samples in statistically independent respect to two classification criteria.
	
	In order to do so we have to create a cross-table where in the rows we put one criteria while on the columns we put the other one; the table is then filled with the cross correlation observations $O_{ij}$. Computing the \textbf{margin totals}
	\[ \hat u_i = \frac 1 n \sum_{j=1}^{c} O_{ij} \qquad \qquad \qquad \hat v_i = \frac 1 n \sum_{i=1}^{r} O_{ij} \]	
	if the two criteria are independent it means that the same values are expected for each criterion, and so the expected value on each cell should be
	\[ E_{ij} = n \hat u_i \hat v_i = \frac 1 n  \sum_{j=1}^{c} \sum_{i=1}^{r} O_{ij}  \]
	
	We can now perform a chi-square test considering the following distribution:
	\begin{equation}
		 X_0^2 = \sum_{j=1}^{c} \sum_{i=1}^{r} \frac{\big(O_{ij}-E_{ij}\big)^2}{E_{ij}} \backsim \mathcal X^2_\nu
	\end{equation}
	on where the number of degrees of freedom $\nu$ of this test is $(r-1)(c-1)$. With that said we can compute the $p$-value of the test statistic as $f_{cd,u,\nu}(X_0^2) = 1- f_{cd,l,\nu}(X_0^2)$; as always the $p$-value refers to the probability of having a type I error, and so if the probability is \textit{low} (in general below a certain threshold) we can confidently accept the strong hypothesis (rejecting the null one), stating so that the two criteria are statistically independent.
	                
	\begin{example}{: contingency test}
		Let's assume the case on where we want to find out is the insurance plans chosen by $500$ workers in a factory depends on their job (that can be salary of hourly); this check can be performed with a contingency test and requires the following table for the actual observation $O_{ij}$:
		               
		\begin{center} 
		\begin{tabular}{ c | c c c | c }
			& \multicolumn{3}{c|}{insurance plan } \\
			job & plan 1 & plan 2 & plan 3 & total \\ \hline
			salary & 160 & 140 & 40 & 340 \\
			hourly & 40 & 60 & 60 & 160 \\ \hline 
			total & 200 & 200 & 100 & 500
		\end{tabular}
		\end{center}
		
		In order to perform the contingency test we need to compute the margin total that for the rows are $\hat u_1 = 340/500 = 0.68$, $\hat u_2 = 160/500 = 0.32$ while for the columns are $\hat v_1 = \hat v_2  = 200 / 500 = 0.4$ and $\hat v_3 = 100/500 = 0.2$. With that said we can compute the table containing the expected observation $E_{ij} = n \hat u_i \hat v_j$ in case of the statistically dependent criteria:
		
		\begin{center} 
			\begin{tabular}{ c | c c c | c }
				& \multicolumn{3}{c|}{insurance plan } \\
				job & plan 1 & plan 2 & plan 3 & total \\ \hline
				salary & 136 & 136 & 68 & 340 \\
				hourly & 64 & 64 & 32 & 160 \\ \hline 
				total & 200 & 200 & 100 & 500
			\end{tabular}
		\end{center}	
		
		At this point we can compute the test statistic as
		\[ X_0 = \sum_{i=1}^2 \sum_{j=1}^3 \frac{\big(O_{ij}-E_{ij}\big)^2}{E_{ij}} = 49.63 \backsim \mathcal X^2_2 \]
		where the degrees of freedom $\nu$ of the chi-square distribution have been calculated as $(r-1)(c-1) = (2-1)(3-1) = 2$. Using so the cumulative density function of the chi square test we can compute the $p$-value as
		\[ \pval = f_{cd,u,\nu}(49.63) = 1 - f_{cd,l,\nu}(49.63) = 1.67 \cdot 10^{-11} \]
		and so we almost have $0\%$ error probability on stating that the two criteria are not statistically dependent (rejecting so the null hypothesis of having statistically dependent criteria).
		
		
	\end{example}
	
	
\subsection{Outliers}
	\de{Outliers} are observation that are not compatible with the distribution of the rest of the sample that may result from error in conducting the measurement or recording the measured value. This is a rare event associated with a different distribution from that typical of the measurements process correctly performed; it is not expected to be more than one outlier per sample (and we cannot extract more than one outlier in order to not alter the original extracted sample). Outliers must be eliminated from the samples and so by doing this we reduce the variance with respect to the expected value; outliers have a stronger effect on smaller samples.
	
	To detect outliers we can use two methods:
	\begin{itemize}
		\item \textbf{Chauvenet's criterion}: given a normally distributed sample $\langle x_1,\dots,x_n\rangle$, given the mean value $\overline x$ and standard variance $s_x$ we have to fined the maximum absolute residual defined as
		\[ r_ 0 = \max_{i=1,\dots,n} \left( \frac{|x_i-\overline x|}{s_x} \right) \]
		At this point the probability of having a value larger then $r_0$ (consider the normal distribution) is that
		\[ P_r = P(x>r_0) = \int_x^\infty f_x(\xi)\, d\xi \]
		using the cumulative distribution function of a standard normal. Over $n$ observation the number of expected observation with residual larger than $r_0$ is $n P_r$ and if this value is less than $0.5$ then the observation with largest residual is eliminated.
		
		\item \textbf{Grubb's test}: this method is based on a statistical test based on the following test statistic:
		\[ s_0 = \max_{i=1,\dots,n} \left( \frac{|x_i-\overline x|}{s_x} \right) \]
		Considering $t_{\alpha,k}$ as the $\alpha$-quantile of a Student's T with $k$ degrees of freedom , then if the inequality
		\[ s_0 > \frac{n-1}{n} \sqrt{\frac{t^2_{\alpha/(2n),n-2}}{n-2+ t^2_{\alpha/(2n), n-2}}} \]
		is verified, then the observation corresponding to $s_0$ is removed from the sample.
		
	\end{itemize}
	In both cases the procedure can be performed only once in order to not alter the initial sample.
	
\section{Regression analyses}
	
	The goal of the regression analysis is to identify the parameters that better adapt a model to a set of observations; the \de{model} is of the type $y = f(x)$ where $y$ is the \textbf{response} (or yield) and $x$ is the \textbf{predictor}.
	
	\begin{SCfigure}[2][bht]
		\centering
		\includegraphics[width=7cm]{regression}
		\caption{example of regression of a quadratic model of type $a x^2 + bx + c$ (where the parameters that needed to be defined are $a,b,c$) on the data shown.}
	\end{SCfigure}
	
	Model can be linear or non-linear (depending on the coefficient that can be changed in order to fit the datas) and mono or multi-dimensional (increasing so the complexity). We define with $n$ the \textbf{rank} of the model as the number of predictors $x_1,\dots, x_n$.
	
	\paragraph{Least squares regression} A way to perform the regression of some datas is by using the \de{leaast square regression} method that's based on the minimization of the performance index $\Phi$ that's the average square residual of the sample $\big\langle x_1,\dots, x_N\big\rangle$:
	\[ \Phi\big(c_1,\dots, c_n\big) = \sum_{i=1}^N \varepsilon_i^2 \]
	where $\varepsilon_i = y_i - f\big(c_1,\dots, c_n, x_i\big)$.
	
	\paragraph{Matrix calculus} Regression of linear models (in the coefficients) can also be performed using matrix calculus notation; considering, as example, the case of a quadratic model in the form $y = ax^2 + bx + c$, we can relate the input with the output as
	\[ \underbrace{\begin{bmatrix}
			x_1^2 & x_1 & 1 \\
			x_2^2 & x_2 & 1 \\
			\vdots & \vdots & \vdots \\
			x_N^2 & x_N & 1 
	\end{bmatrix} }_A \begin{pmatrix}
		a\\b\\c
	\end{pmatrix} = \begin{pmatrix}
		y_1 \\ y_2 \\ \vdots \\ y_N
	\end{pmatrix}\]
	
	
\subsection{Prediction and confidence intervals}
	The regression model shall also provide information regarding:
	\begin{itemize}
		\item the \de{prediction interval}, and so the \textit{range} with a given probability (usually $95\%$ or higher) on which the model will fall with a certain \textit{correctness};
		
		\item the \de{confidence interval}, associated to the \textit{range} on which future observation will fall with a given probability; for the same probability, this interval is wider than the prediction one. 
	\end{itemize}
	\begin{SCfigure}[2][bht]
		\centering
		\includegraphics[width=7cm]{intervals}
		\caption{prediction and confidence interval for linearly fitted data.}
	\end{SCfigure}
	
\subsection{Under and over-fitting}
	
	\de{Under-fitting} occurs every time the model used to fit some data present \textit{lower number} of parameters that doesn't allow to \textit{correctly} describe the phenomena of interest (the chosen model \textit{is not curved enough}), as in case of figure \ref{fig:meas:underfit}. In this case when plotting the graph of the residuals it's possible to note some patters, stressing out that the model wasn't allowed to perfectly fit the data.
	
	\begin{figure}[bht]
		\centering
		\begin{subfigure}{0.48\linewidth}
			\centering 
			\includegraphics[width=6cm]{of-1} \caption{}
		\end{subfigure}
		\begin{subfigure}{0.48\linewidth}
			\centering 
			\includegraphics[width=6cm]{of-2} \caption{}
		\end{subfigure}
		\caption{example of linear fit of "quadratic distributed" data (a) and related residuals plot (b).} \label{fig:meas:underfit}
	\end{figure}
	
	Completely different is the case of \de{over-fitting}, where the model used for the data fit is \textit{more curve than required}, as in figure \ref{fig:meas:overfit}. In this case, looking at the residual plot gives no idea on the type of error (in fact the residuals will look randomly distributed). The only way to find out this problem is to fit the model only on the \textit{inner} bound of the sample: if out of this region the confidence/prediction interval diverges quickly, then this means that the chosen model over-fits the data.
	
	\begin{SCfigure}[2][bht]
		\centering
		\includegraphics[width=6cm]{of-3}
		\caption{example of "quadratic distributed" data fitted with a grade 9 polynomial. In this case the fit is made only with values in the domain $[1,9]$.} \label{fig:meas:overfit}
	\end{SCfigure}